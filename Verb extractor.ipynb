{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('polizeiberichte.pkl', 'rb') as f:\n",
    "    data = pickle.load(f, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def extract_verb_ver1(corpus):\n",
    "    sentence_extract = defaultdict(dict)\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        first_propn = \"\"\n",
    "        second_propn = \"\"\n",
    "        verb = \"\"\n",
    "        pron = \"\"\n",
    "        for word in sentence:\n",
    "            if word[\"upostag\"] == \"PROPN\":\n",
    "                if first_propn == \"\":\n",
    "                    first_propn = word[\"form\"]\n",
    "                elif second_propn == \"\":\n",
    "                    second_propn = word[\"form\"]\n",
    "            if word[\"upostag\"] == \"VERB\":\n",
    "                if verb == \"\" and first_propn != \"\" and second_propn == \"\":\n",
    "                    verb = word[\"form\"]\n",
    "            if word[\"upostag\"] == \"PRON\":\n",
    "                if verb != \"\" and first_propn != \"\" and second_propn != \"\" and pron == \"\":\n",
    "                    pron = word[\"form\"]\n",
    "        if first_propn != \"\" and second_propn != \"\" and verb != \"\" and pron != \"\":\n",
    "            sentence_extract[i][\"first\"] = first_propn\n",
    "            sentence_extract[i][\"verb\"] = verb\n",
    "            sentence_extract[i][\"second\"] = second_propn\n",
    "            sentence_extract[i][\"pron\"] = pron\n",
    "            sentence_extract[i][\"sentence\"] = [w[\"form\"] for w in sentence]\n",
    "    return sentence_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t0,00001% der Stimmrechte (das entspricht 14 Stimmrechten) sind der Gesellschaft gemäß § 22 Abs. 1, Satz 1, Nr. 2 in Verbindung mit Satz 2 WpHG zuzurechnen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy_conll import ConllFormatter\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conllformatter = ConllFormatter(nlp)\n",
    "nlp.add_pipe(conllformatter, after='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = data[1][\"text1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(example_text)\n",
    "\n",
    "conll = doc._.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 2):\n",
    "    example_text = data[i][\"text1\"]\n",
    "    if example_text:\n",
    "        doc = nlp(example_text)\n",
    "        conll = doc._.conll\n",
    "        extraction = extract_verb_ver1(conll)\n",
    "        if extraction:\n",
    "            print(extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[OrderedDict([('id', 1), ('form', 'Alex'), ('lemma', 'Alex'), ('upostag', 'PROPN'), ('xpostag', 'NE'), ('feats', '_'), ('head', 2), ('deprel', 'sb'), ('deps', '_'), ('misc', '_')]), OrderedDict([('id', 2), ('form', 'sagt'), ('lemma', 'sagen'), ('upostag', 'VERB'), ('xpostag', 'VVFIN'), ('feats', 'Mood=ind|VerbForm=fin'), ('head', 0), ('deprel', 'ROOT'), ('deps', '_'), ('misc', '_')]), OrderedDict([('id', 3), ('form', 'chris'), ('lemma', 'chris'), ('upostag', 'PROPN'), ('xpostag', 'NE'), ('feats', '_'), ('head', 2), ('deprel', 'oa'), ('deps', '_'), ('misc', 'SpaceAfter=No')]), OrderedDict([('id', 4), ('form', ','), ('lemma', ','), ('upostag', 'PUNCT'), ('xpostag', '$,'), ('feats', 'PunctType=comm'), ('head', 2), ('deprel', 'punct'), ('deps', '_'), ('misc', '_')]), OrderedDict([('id', 5), ('form', 'dass'), ('lemma', 'dass'), ('upostag', 'SCONJ'), ('xpostag', 'KOUS'), ('feats', '_'), ('head', 9), ('deprel', 'cp'), ('deps', '_'), ('misc', '_')]), OrderedDict([('id', 6), ('form', 'er'), ('lemma', 'ich'), ('upostag', 'PRON'), ('xpostag', 'PPER'), ('feats', 'PronType=prs'), ('head', 9), ('deprel', 'sb'), ('deps', '_'), ('misc', '_')]), OrderedDict([('id', 7), ('form', 'essen'), ('lemma', 'essen'), ('upostag', 'VERB'), ('xpostag', 'VVINF'), ('feats', 'VerbForm=inf'), ('head', 8), ('deprel', 'oa'), ('deps', '_'), ('misc', '_')]), OrderedDict([('id', 8), ('form', 'machen'), ('lemma', 'machen'), ('upostag', 'VERB'), ('xpostag', 'VVINF'), ('feats', 'VerbForm=inf'), ('head', 9), ('deprel', 'oc'), ('deps', '_'), ('misc', '_')]), OrderedDict([('id', 9), ('form', 'soll'), ('lemma', 'soll'), ('upostag', 'VERB'), ('xpostag', 'VMFIN'), ('feats', 'Mood=ind|VerbForm=fin|VerbType=mod'), ('head', 2), ('deprel', 'oc'), ('deps', '_'), ('misc', 'SpaceAfter=No')])]]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Alex sagt chris, dass er essen machen soll\")\n",
    "conll = doc._.conll\n",
    "print(conll)\n",
    "#extract_verb_ver1(conll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nearest_propn(conll, index):\n",
    "    distance_dict = {}\n",
    "    left = None\n",
    "    right = None\n",
    "    for i in range(index - 1, -1, -1):\n",
    "        if conll[i][\"upostag\"] == \"PROPN\":\n",
    "            left = (index - i)*-1\n",
    "            break\n",
    "    for i in range(index + 1, len(conll)):\n",
    "        if conll[i][\"upostag\"] == \"PROPN\":\n",
    "            right = i - index\n",
    "            break\n",
    "    if left and right:\n",
    "        if abs(left + right) < 3:\n",
    "            return (left, right)\n",
    "   \n",
    "\n",
    "def get_verb_dict(corpus):\n",
    "    sentence_extract = defaultdict(dict)\n",
    "    verb_dict = defaultdict(dict)\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word[\"upostag\"] == \"VERB\":\n",
    "                nearest_propn = _get_nearest_propn(sentence, j)\n",
    "                if nearest_propn:\n",
    "                    verb_dict[i][word[\"form\"]] = (sentence[j+nearest_propn[0]][\"form\"], sentence[j+nearest_propn[1]][\"form\"])\n",
    "    return verb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict, {0: {'sagt': ('Alex', 'chris')}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_verb_dict(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 defaultdict(<class 'dict'>, {2: {'erfasste': ('Allee', 'BMW')}})\n",
      "29 defaultdict(<class 'dict'>, {1: {'bog': ('Heinrich-Mann-Straße', 'Hermann-Hesse-Straße')}})\n",
      "128 defaultdict(<class 'dict'>, {1: {'erhob': ('Uhlandstraße', 'Heil')}})\n",
      "135 defaultdict(<class 'dict'>, {12: {'bemerkten': ('Schmidstraße', 'VW')}})\n",
      "153 defaultdict(<class 'dict'>, {1: {'überholt': ('Damms', 'Rhenaniastraße')}})\n",
      "159 defaultdict(<class 'dict'>, {2: {'bremsen': ('Renault', 'VW')}})\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 200):\n",
    "    example_text = data[i][\"text1\"]\n",
    "    if example_text:\n",
    "        doc = nlp(example_text)\n",
    "        conll = doc._.conll\n",
    "        extraction = get_verb_dict(conll)\n",
    "        if extraction:\n",
    "            print(i, extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Die Polizei Berlin sucht dringend Zeugen zu einem Verkehrsunfall vom vergangenen Samstag, 23. August 2014, in Marzahn. Wie berichtet, fuhren ein „Nissan“ und ein „BMW“ gegen 21.50 Uhr an der Kreuzung Landsberger Allee/Blumberger Damm in Fahrtrichtung stadtauswärts bei „Grün“ sehr rasant mit aufheulenden Motorengeräuschen an. Vor der Hausnummer 563 in der Landsberger Allee erfasste der „BMW“ eine Radfahrerin, die nach den bisherigen Erkenntnissen bei „Rot“ die Fußgängerampel überquerte. Bei dem Unfall erlitt die 27-jährige Radfahrerin lebensbedrohliche Verletzungen und wird derzeit weiterhin intensivmedizinisch betreut.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[25][\"text1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x09'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-98fe2805586c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'deu_news_2015_3M-sentences.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mger_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x09'."
     ]
    }
   ],
   "source": [
    "with open('deu_news_2015_3M-sentences.txt', 'rb') as f:\n",
    "    f.seek(1)\n",
    "    ger_sent = pickle.load(f, encoding=\"utf8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file1 = open('deu_news_2015_3M-sentences.txt', 'r') \n",
    "Lines = file1.readlines() \n",
    "print(Lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_lines = Lines[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 defaultdict(<class 'dict'>, {1: {'kappt': ('Serbien', 'Kroatien')}})\n",
      "28 defaultdict(<class 'dict'>, {0: {'gibt': ('Litauen', 'Russland')}})\n",
      "38 defaultdict(<class 'dict'>, {1: {'filmt': ('Altaussee', 'Bond-Crew')}})\n",
      "46 defaultdict(<class 'dict'>, {0: {'gibt': ('Liverpool', 'Boxing')}})\n",
      "54 defaultdict(<class 'dict'>, {1: {'umfassen': ('DGAP', 'Corporate')}})\n",
      "68 defaultdict(<class 'dict'>, {1: {'identifiziert': ('Interpol', 'Irak')}})\n",
      "76 defaultdict(<class 'dict'>, {2: {'gehen': ('ots', 'Deutschland')}})\n",
      "77 defaultdict(<class 'dict'>, {0: {'übermittelt': ('News/Finanznachricht', 'DGAP')}})\n",
      "87 defaultdict(<class 'dict'>, {1: {'umfassen': ('DGAP', 'Corporate')}})\n",
      "89 defaultdict(<class 'dict'>, {2: {'entgleisen': ('Gerd', '@balli')}})\n",
      "93 defaultdict(<class 'dict'>, {4: {'beschlagnahmen': ('Hamburg', 'Ulrich')}})\n",
      "110 defaultdict(<class 'dict'>, {1: {'einzuschüchtern': ('Russlands', 'Wladimir')}})\n",
      "113 defaultdict(<class 'dict'>, {2: {'schließt': ('IWF', 'Athens')}})\n",
      "119 defaultdict(<class 'dict'>, {1: {'schmieden': ('USA', 'China'), 'wollen': ('China', 'Russland'), 'schreibt': ('Südostasien', 'Nesawissimaja')}})\n",
      "120 defaultdict(<class 'dict'>, {0: {'schmieden': ('USA', 'China'), 'wollen': ('China', 'Russland'), 'schreibt': ('Südostasien', 'Nesawissimaja')}})\n",
      "124 defaultdict(<class 'dict'>, {2: {'schließt': ('IWF', 'Athens')}})\n",
      "127 defaultdict(<class 'dict'>, {1: {'schmieden': ('USA', 'China'), 'wollen': ('China', 'Russland'), 'schreibt': ('Südostasien', 'Nesawissimaja')}})\n",
      "128 defaultdict(<class 'dict'>, {1: {'schmieden': ('USA', 'China'), 'wollen': ('China', 'Russland'), 'schreibt': ('Südostasien', 'Nesawissimaja')}})\n",
      "134 defaultdict(<class 'dict'>, {2: {'schließt': ('IWF', 'Athens')}})\n",
      "155 defaultdict(<class 'dict'>, {1: {'muss': ('Fuller', 'Lewis')}})\n",
      "170 defaultdict(<class 'dict'>, {0: {'verlieren': ('Shaq', 'Sampdoria')}})\n",
      "179 defaultdict(<class 'dict'>, {3: {'überrascht': ('Dungeons', 'Blizzard')}})\n",
      "181 defaultdict(<class 'dict'>, {3: {'übermittelt': ('Quartalsfinanzbericht', 'adhoc')}})\n",
      "190 defaultdict(<class 'dict'>, {2: {'schließt': ('IWF', 'Athens')}})\n",
      "191 defaultdict(<class 'dict'>, {1: {'umfassen': ('DGAP', 'Corporate')}})\n",
      "193 defaultdict(<class 'dict'>, {2: {'gezogen': ('of', 'Montreal')}})\n",
      "195 defaultdict(<class 'dict'>, {2: {'unterschreibt': ('Grillitsch', 'Bremen')}})\n",
      "197 defaultdict(<class 'dict'>, {1: {'bekommen': ('Großbritannien', 'Juri')}})\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 200):\n",
    "    example_text = ger_lines[i]\n",
    "    if example_text:\n",
    "        doc = nlp(example_text)\n",
    "        conll = doc._.conll\n",
    "        extraction = get_verb_dict(conll)\n",
    "        if extraction:\n",
    "            print(i, extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\t01498 USA schmieden Bündnis gegen China und wollen Russland ausbootenWashington bemüht sich um eine antichinesische Koalition in Südostasien, schreibt die \"Nesawissimaja Gaseta\" am Dienstag.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Lines[119])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
