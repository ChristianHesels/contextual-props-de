{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, codecs, time, datetime\n",
    "import fileinput\n",
    "import os.path\n",
    "import pickle\n",
    "from io import StringIO\n",
    "from subprocess import call\n",
    "\n",
    "\n",
    "from docopt import docopt\n",
    "from propsde.applications.viz_tree import DepTreeVisualizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import propsde.applications.run as run\n",
    "import sys\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str\n",
    "    \n",
    "import spacy\n",
    "from spacy_conll import ConllFormatter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CorZu 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "import subprocess\n",
    "\n",
    "def _get_spacy_nlp():\n",
    "    spacy.prefer_gpu()\n",
    "    nlp = spacy.load('de_core_news_md')\n",
    "    conllformatter = ConllFormatter(nlp)\n",
    "    nlp.add_pipe(conllformatter, after='parser')\n",
    "    return nlp\n",
    "\n",
    "def _get_stanza_nlp():\n",
    "    nlp = stanza.Pipeline('de')\n",
    "    return nlp\n",
    "\n",
    "def _parse_dependencies_to_conll_file_stanza(text, nlp, conll_name):\n",
    "    doc = nlp(text).to_dict() \n",
    "    conll = CoNLL.convert_dict(doc)\n",
    "    conll_str = \"\"\n",
    "    for sentence in conll:\n",
    "        for word in sentence:\n",
    "            for attr in word:\n",
    "                conll_str += attr + \"\\t\"\n",
    "            conll_str += \"\\n\"\n",
    "            conll_str = conll_str.replace(\"\\t\\n\", \"\\n\")\n",
    "    try:\n",
    "        file = open(conll_name,'w')\n",
    "        file.write(conll_str)\n",
    "        return 0\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return 1\n",
    "    \n",
    "def _write_to_file(text, file_name):\n",
    "    try:\n",
    "        file = open(file_name,'w')\n",
    "        file.write(text)\n",
    "        return 0\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return 1\n",
    "    \n",
    "def _parse_dependencies_parzu_http(text):\n",
    "    params = {\n",
    "        \"text\": text,\n",
    "        \"format\": \"conll\"\n",
    "    }\n",
    "    args = urllib.parse.urlencode(params).encode(\"utf-8\")\n",
    "    req = \"http://localhost:5003/parse/?\"+args.decode(\"utf-8\")\n",
    "    try:\n",
    "        f = urllib.request.urlopen(req)\n",
    "        return f.read().decode(\"utf-8\")\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return None \n",
    "\n",
    "def _parse_dependencies_to_conll_file_parzu_http(text, conll_name):\n",
    "    params = {\n",
    "        \"text\": text,\n",
    "        \"format\": \"conll\"\n",
    "    }\n",
    "    args = urllib.parse.urlencode(params).encode(\"utf-8\")\n",
    "    req = \"http://localhost:5003/parse/?\"+args.decode(\"utf-8\")\n",
    "    try:\n",
    "        f = urllib.request.urlopen(req)\n",
    "        _write_to_file(f, conll_name)\n",
    "        return 0\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return 1\n",
    "    \n",
    "def _parse_dependencies_to_conll_file_parzu(text, conll_name):\n",
    "    cmd = \"echo '\" + text + \"' | docker run -i rsennrich/parzu /ParZu/parzu\"\n",
    "    process = subprocess.Popen(['docker','run', \"-i\", \"rsennrich/parzu\", \"/ParZu/parzu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    data = process.communicate()\n",
    "    print(data[1].decode('utf-8'))\n",
    "    \n",
    "def _parse_dependencies_to_conll_file_spacy(text, nlp, conll_name):\n",
    "    doc = nlp(text)\n",
    "    conll = doc._.conll_str\n",
    "    print(conll)\n",
    "    conll = conll.replace(\"\\n\\n\", \"\\n\")\n",
    "    _write_to_file(conll, conll_name)\n",
    "\n",
    "def _create_markables(conll_name, markable_name):\n",
    "    cmd = \"python ext/CorZu_v2.0/extract_mables_from_parzu.py \" + conll_name + \" > \" + markable_name\n",
    "    err = os.system(cmd)\n",
    "    return err\n",
    "\n",
    "def execute_corzu(text, nlp=None, conll_name=\"output.conll\", markable_name=\"markable\", output=\"corzu_export\"):\n",
    "    if nlp:\n",
    "        print(\"Using Stanza\")\n",
    "        #_parse_dependencies_to_conll_file_spacy(text, nlp, conll_name)\n",
    "        _parse_dependencies_to_conll_file_stanza(text, nlp, conll_name)\n",
    "    else:\n",
    "        print(\"Using ParZu\")\n",
    "        _parse_dependencies_to_conll_file_parzu(text, conll_name)\n",
    "    err = _create_markables(conll_name, markable_name)\n",
    "    if err == 0:\n",
    "        cmd = \"python ext/CorZu_v2.0/corzu.py \" + markable_name + \" \" + conll_name + \" \" + output    \n",
    "        err = os.system(cmd)\n",
    "        return err\n",
    "    else:\n",
    "        print(\"Markables Error:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Tuebadz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def read_tuebadz_per_sentence(path):\n",
    "    i = 0\n",
    "    raw_data = {}\n",
    "    gold_standard = {}\n",
    "    sentences = defaultdict(list)\n",
    "    for line in open(path,'r').readlines():\n",
    "        li=line.strip()\n",
    "        if li.startswith(\"#\"):\n",
    "            li = li.split()\n",
    "            if li[1] == 'newdoc':\n",
    "                new_doc = li[4]\n",
    "                gold_standard[new_doc] = \"\"\n",
    "            if li[1] == 'sent_id':\n",
    "                sent_id = li[3]\n",
    "            if li[1] == 'text':\n",
    "                sentences[new_doc].append(line[9:].rstrip(\" \\n\"))\n",
    "        else:\n",
    "            gold_standard[new_doc] += line\n",
    "        #if i == 1000:\n",
    "        #    break\n",
    "        #i += 1\n",
    "    sentences[new_doc].append(line[9:].rstrip(\" \\n\"))\n",
    "    return sentences, gold_standard\n",
    "            \n",
    "\n",
    "raw_sentences, gold_standard = read_tuebadz_per_sentence(\"tuebadz-11.0-conll-u-v2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = _get_spacy_nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_parzu():\n",
    "    i = 0\n",
    "    conll = \"\"\n",
    "    for new_doc in raw_sentences:\n",
    "        text = raw_sentences[new_doc]\n",
    "        conll += \"\\n#begin\\n\"\n",
    "        for s in text:\n",
    "            try:\n",
    "                conll += _parse_dependencies_parzu_http(s)\n",
    "            except Exception as ex:\n",
    "                print(new_doc)\n",
    "                print(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output = \"corzu_export\"\n",
    "markables = \"markables\"\n",
    "text = open(\"examples.txt\", \"r\").read()\n",
    "\n",
    "cmd = \"python ext/CorZu_v2.0/extract_mables_from_parzu.py \" + conll + \" > \" + markables\n",
    "err = os.system(cmd)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ParZu\n",
      "HTTP Error 408: REQUEST TIMEOUT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(raw_sentences[0][\"text\"])\n",
    "execute_corzu(raw_sentences[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ParZu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_corzu(\"Peter geht spazieren, er mag Pommes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-30 12:25:50 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | conll03 |\n",
      "=======================\n",
      "\n",
      "2020-06-30 12:25:50 INFO: Use device: gpu\n",
      "2020-06-30 12:25:50 INFO: Loading: tokenize\n",
      "2020-06-30 12:25:52 INFO: Loading: mwt\n",
      "2020-06-30 12:25:52 INFO: Loading: pos\n",
      "2020-06-30 12:25:53 INFO: Loading: lemma\n",
      "2020-06-30 12:25:53 INFO: Loading: depparse\n",
      "2020-06-30 12:25:54 INFO: Loading: ner\n",
      "2020-06-30 12:25:54 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stanza\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = _get_stanza_nlp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Stanza\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_corzu(\"Peter geht spazieren, er mag Pommes\", nlp, conll_name=\"output_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Das der PRON PDS Case=Nom|Gender=Neut|Number=Sing|PronType=Dem 4 nsubj _ start_char=0|end_char=3 \n",
      "2 ist sein AUX VAFIN Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin 4 cop _ start_char=4|end_char=7 \n",
      "3 ein ein DET ART Case=Nom|Definite=Ind|Gender=Masc|Number=Sing|PronType=Art 4 det _ start_char=8|end_char=11 \n",
      "4 Test Test NOUN NN Case=Nom|Gender=Masc|Number=Sing 0 root _ start_char=12|end_char=16 \n",
      "5 . . PUNCT $. _ 4 punct _ start_char=16|end_char=17 \n",
      "1 Er er PRON PPER Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs 3 nsubj _ start_char=18|end_char=20 \n",
      "2 ist sein AUX VAFIN Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin 3 cop _ start_char=21|end_char=24 \n",
      "3 Gut Gut NOUN NN Case=Nom|Gender=Neut|Number=Sing 0 root _ start_char=25|end_char=28 \n",
      "4 . . PUNCT $. _ 3 punct _ start_char=28|end_char=29 \n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
