{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, codecs, time, datetime\n",
    "import fileinput\n",
    "import os.path\n",
    "import stanza\n",
    "from io import StringIO\n",
    "from subprocess import call\n",
    "\n",
    "\n",
    "from docopt import docopt\n",
    "from propsde.applications.viz_tree import DepTreeVisualizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import propsde.applications.run as run\n",
    "import sys\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str\n",
    "    \n",
    "#stanza.download('de')\n",
    "#nlp = stanza.Pipeline('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('polizeiberichte.pkl', 'rb') as f:\n",
    "    data = pickle.load(f, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gute Texte: 5\n",
    "Schwierige Texte: 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am Donnerstag, dem 27. Februar gegen 10:30 Uhr, hatten zunächst zwei Männer das Juweliergeschäft „Stöger“ in der Berkaer Straße 6 im Ortsteil Schmargendorf betreten. Sie drängten den 61-jährigen Mitinhaber unter Drohung mit einer Schusswaffe in die hinteren Räume, brachten ihn zu Boden, schlugen mit dem Pistolengriff mehrmals auf seinen Kopf ein und fesselten ihn anschließend. Fast gleichzeitig betrat ein Komplize mit einer Sporttasche das Geschäft. Die drei Täter rafften nunmehr aus den Vitrinen und Auslagen hochwertigen Schmuck sowie insgesamt fünfzehn Armbanduhren zusammen und flüchteten anschließend in unbekannte Richtung.\n"
     ]
    }
   ],
   "source": [
    "text_example = data[11][\"text1\"]\n",
    "print(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = codecs.open(\"examples.txt\", encoding='utf8')\n",
    "gs = run.parseSentences(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph, tree in gs:\n",
    "    original_line = str(graph).split(\"\\n\")[0].split(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zwei\n",
      "(0\n",
      "Männer\n",
      "0)\n",
      "das\n",
      "(2\n",
      "Stöger\n",
      "2)\n",
      "Sie\n",
      "(0)\n",
      "den\n",
      "(1\n",
      "Mitinhaber\n",
      "1)\n",
      "ihn\n",
      "(1)\n",
      "seinen\n",
      "(1)\n",
      "ihn\n",
      "(1)\n",
      "das\n",
      "(2\n",
      "Geschäft\n",
      "2)\n"
     ]
    }
   ],
   "source": [
    "for graph, tree in gs:\n",
    "    for node in graph:\n",
    "        if node.coreference:\n",
    "            print(node.text[0])\n",
    "            print(node.coreference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParZU Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "def parse_dependencies_to_conll_file(text, path):\n",
    "    params = {\n",
    "        \"text\": text,\n",
    "        \"format\": \"conll\"\n",
    "    }\n",
    "    args = urllib.parse.urlencode(params).encode(\"utf-8\")\n",
    "    req = \"http://localhost:5003/parse/?\"+args.decode(\"utf-8\")\n",
    "    try:\n",
    "        f = urllib.request.urlopen(req)\n",
    "        if path:\n",
    "            file = open(path,'w')\n",
    "            file.write(f.read().decode('utf-8'))\n",
    "        else:\n",
    "            print(f.read().decode('utf-8'))\n",
    "        return 0\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return 1\n",
    "    \n",
    "parse_dependencies_to_conll_file(\"Alex sagt Chris\", \"test2.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CorZu 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output = \"corzu_export\"\n",
    "conll = \"test.conll\"\n",
    "markables = \"markables\"\n",
    "text = open(\"examples.txt\", \"r\").read()\n",
    "\n",
    "err = parse_dependencies_to_conll_file(text_example, conll)\n",
    "if err == 0:\n",
    "    cmd = \"python ext/CorZu_v2.0/extract_mables_from_parzu.py \" + conll + \" > \" + markables\n",
    "    err = os.system(cmd)\n",
    "    print(err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "cmd = \"python ext/CorZu_v2.0/corzu.py \" + markables + \" \" + conll + \" \" + output    \n",
    "err = os.system(cmd)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tPaul\tPaul\tN\tNE\tMasc|Nom|Sg\t2\tsubj\t_\t_\n",
      "2\tsagt\tsagen\tV\tVVFIN\t3|Sg|Pres|Ind\t0\troot\t_\t_\n",
      "3\tzu\tzu\tPREP\tAPPR\tDat\t2\tpp\t_\t_\n",
      "4\tErik\tErik\tN\tNE\t_|Dat|_\t3\tpn\t_\t_\n",
      "5\t,\t,\t$,\t$,\t_\t0\troot\t_\t_\n",
      "6\tdass\tdass\tKOUS\tKOUS\t_\t11\tkonj\t_\t_\n",
      "7\ter\ter\tPRO\tPPER\t3|Sg|Masc|Nom\t11\tsubj\t_\t_\n",
      "8\tden\tdie\tART\tART\tDef|Masc|Acc|Sg\t9\tdet\t_\t_\n",
      "9\tSchlüssel\tSchlüssel\tN\tNN\tMasc|Acc|Sg\t10\tobja\t_\t_\n",
      "10\tvergessen\tveressen\tV\tVVPP\t_\t11\taux\t_\t_\n",
      "11\that\thaben\tV\tVAFIN\t3|Sg|Pres|Ind\t2\tobjc\t_\t_\n",
      "\n",
      "\n",
      "1\tPaul\tPaul\tN\tNE\tMasc|Nom|Sg\t2\tsubj\t_\t_\n",
      "2\tsagt\tsagen\tV\tVVFIN\t3|Sg|Pres|Ind\t0\troot\t_\t_\n",
      "3\tzu\tzu\tPREP\tAPPR\tDat\t2\tpp\t_\t_\n",
      "4\tErik\tErik\tN\tNE\t_|Dat|_\t3\tpn\t_\t_\n",
      "5\t,\t,\t$,\t$,\t_\t0\troot\t_\t_\n",
      "6\tdass\tdass\tKOUS\tKOUS\t_\t0\troot\t_\t_\n",
      "7\ter\ter\tPRO\tPPER\t3|Sg|Masc|Nom\t0\troot\t_\t_\n",
      "8\tzurück\tzurück\tPTKVZ\tPTKVZ\t_\t0\troot\t_\t_\n",
      "9\tgehen\tgehen\tV\tVVINF\t_\t10\taux\t_\t_\n",
      "10\tsollte\tsollen\tV\tVMFIN\t_|Sg|Past|_\t0\troot\t_\t_\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_dependencies_to_conll_file(\"Paul sagt zu Erik, dass er den Schlüssel vergessen hat\", None)\n",
    "parse_dependencies_to_conll_file(\"Paul sagt zu Erik, dass er zurück gehen sollte\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with CorZu Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_np(lines, id):\n",
    "    np = \"\"\n",
    "    indices = 0\n",
    "    for line in lines:\n",
    "        line_list = re.split('\\t| +',line)\n",
    "        np += line_list[1] + \" \"\n",
    "        indices += 1\n",
    "        if line.strip().endswith(id + ')'):\n",
    "            return np, indices\n",
    "\n",
    "\n",
    "text = open(output, \"r\").read()\n",
    "lines=text.split('\\n\\n')\n",
    "\n",
    "for i in range(0, len(lines)):\n",
    "    lines[i] = lines[i].split(\"\\n\")\n",
    "    for j in range(0, len(lines[i])):\n",
    "        lines[i][j] = lines[i][j].split(\"\\t\")\n",
    "lines = {i: l for i, l in enumerate(lines) if len(l) > 0}\n",
    "\n",
    "\n",
    "for sentence_count, gt in enumerate(gs):\n",
    "    graph, tree = gt\n",
    "    word_ids = []\n",
    "    for node in graph.nodes():\n",
    "        index = int(node.to_conll_like().split(\"\\t\")[1].split(\",\")[0]) -1\n",
    "        if lines[sentence_count][index][-1] != \"-\" and lines[sentence_count][index][-1] != \"\":\n",
    "            node.features[\"coreference\"] = lines[sentence_count][index][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "\n",
    "i = 0\n",
    "for graph, tree in gs:\n",
    "    dot = graph.writeToDot(None, None)\n",
    "\n",
    "    s = Source(dot, filename=str(i) + \"_test.gv\", format=\"png\")\n",
    "    s.view()\n",
    "    i += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEO4J Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "\n",
    "def _create_node(tx, uid, label, coreference):\n",
    "    tx.run(\"CREATE (e:Node {label: $label, uid: $uid, coreference: $coreference}) RETURN e\", label=label, uid=uid, coreference=coreference)\n",
    "    \n",
    "def _create_edge(tx, src, dst, label):\n",
    "    tx.run(\"MATCH (src:Node), (dst:Node) WHERE src.uid = $src AND dst.uid = $dst MERGE (src)-[r:edge {label: $label}]->(dst)\", src=src, dst=dst, label=label)\n",
    "    \n",
    "def _delete_all(tx):\n",
    "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "def create_node(driver, uid, label, coreference=None):\n",
    "    with driver.session() as session:\n",
    "        session.write_transaction(_create_node, uid, label, coreference)\n",
    "            \n",
    "def create_edge(driver, src, dst, label):\n",
    "    with driver.session() as session:\n",
    "        session.write_transaction(_create_edge, src, dst, label)\n",
    "    \n",
    "def delete_all(driver):\n",
    "    with driver.session() as session:\n",
    "        session.write_transaction(_delete_all)\n",
    "        \n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygraph.classes.digraph import digraph\n",
    "from collections import defaultdict\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"852963\"))\n",
    "\n",
    "coreference_map = defaultdict(list)\n",
    "\n",
    "coreference_head = {}\n",
    "\n",
    "def _is_part_of_initial_coreference(nodes, uid):\n",
    "    current_node = nodes[uid]\n",
    "    if current_node.coreference:\n",
    "        coref_id = int(re.findall(r'\\d+', current_node.coreference)[0])\n",
    "\n",
    "        if coref_id not in coreference_map:\n",
    "            coreference_map[coref_id].append(current_node.coreference)\n",
    "            return True\n",
    "        else:\n",
    "            is_finished = False\n",
    "            for coref in coreference_map[coref_id]:\n",
    "                if coref[-1] == \")\":\n",
    "                    is_finished = True\n",
    "            if not is_finished:\n",
    "                coreference_map[coref_id].append(current_node.coreference)\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "                    \n",
    "def write_graphs_to_neo4j(driver, graphs):\n",
    "    delete_all(driver)\n",
    "    for graph, tree in graphs:\n",
    "        nodes = graph.nodesMap\n",
    "        for uid in nodes:\n",
    "            node = nodes[uid]\n",
    "            # NODE\n",
    "            label = str(node.text[0])\n",
    "            if node.isPredicate:\n",
    "                # is predicate\n",
    "                pass\n",
    "            if node.features.get(\"top\", False):\n",
    "                # is top\n",
    "                pass\n",
    "            \n",
    "            # Remove Punctuation Marks\n",
    "            if len(label) > 1:\n",
    "                if node.coreference: and _is_part_of_initial_coreference(nodes, uid):\n",
    "                    create_node(driver, uid, label, node.coreference)\n",
    "                elif not node.coreference:\n",
    "                    create_node(driver, uid, label, node.coreference)\n",
    "            \n",
    "            # EDGE\n",
    "            for (src, dst) in digraph.edges(graph):\n",
    "                label = str(graph.edge_label((src, dst)))\n",
    "                if label:\n",
    "                    coref_id = None\n",
    "                    if nodes[src].coreference:\n",
    "                        coref_id = int(re.findall(r'\\d+', nodes[src].coreference)[0])\n",
    "                        coreference_head[coref_id] = src\n",
    "                        create_edge(driver, coreference_head.get(coref_id, src), dst, label) \n",
    "                    elif nodes[dst].coreference:\n",
    "                        coref_id = int(re.findall(r'\\d+', nodes[dst].coreference)[0])\n",
    "                        create_edge(driver, src, coreference_head.get(coref_id, dst), label) \n",
    "                    else:\n",
    "                        create_edge(driver, src, dst, label) \n",
    "\n",
    "    \n",
    "#delete_all(driver)\n",
    "write_graphs_to_neo4j(driver, gs)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zwei (0\n",
      "Männer 0)\n",
      "das (2\n",
      "Stöger 2)\n",
      "Sie (0)\n",
      "den (1\n",
      "Mitinhaber 1)\n",
      "ihn (1)\n",
      "seinen (1)\n",
      "ihn (1)\n",
      "das (2\n",
      "Geschäft 2)\n"
     ]
    }
   ],
   "source": [
    "for graph, tree in gs:\n",
    "    for node in graph:\n",
    "        if node.coreference:\n",
    "            print(node.text[0], node.coreference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ad643d169348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentence_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "sentence_index = 0\n",
    "for graph, tree in gs:\n",
    "    sentences[sentence_index] = {}\n",
    "    for line in lines:\n",
    "            l = line.split('\\t')\n",
    "            line_list = re.split('\\t| +',line)\n",
    "\n",
    "            id = re.findall('\\d+',line_list[-1])\n",
    "            if id and id[0] not in entities and line.strip().endswith('(' + id[0]):\n",
    "    \n",
    "                sentences[sentence_index][l[0]] = id[0]\n",
    "                print(i, l[0], l[1])\n",
    "                entities[i] = \"(\" + id[0]\n",
    "    \n",
    "                for j in range(1, indices-2):\n",
    "         \n",
    "                    entities[i + j] = \"-\"\n",
    "                entities[i + indices] = id[0] + \")\"\n",
    "    \n",
    "            elif id and line.strip().endswith('(' + id[0] + ')'):\n",
    "                entities[i] = \"(\" + id[0] + \")\"\n",
    "    sentence_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining CorZu and PropsDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE WITH CORZU AS INPUT FILE\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract propositions with words and their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import product\n",
    "from propsde.graph_representation.graph_utils import get_min_max_span, find_nodes, \\\n",
    "    find_edges, merge_nodes, multi_get, duplicateEdge, accessibility_wo_self, \\\n",
    "    replace_head, find_marker_idx\n",
    "from propsde.utils.utils import encode_german_characters, encode_german_chars\n",
    "from propsde.graph_representation.proposition import Proposition\n",
    "from propsde.graph_representation.word import NO_INDEX, Word, strip_punctuations\n",
    "\n",
    "def subgraph_to_string(graph,node,exclude=[]):\n",
    "    nodes = [node]\n",
    "    change = True\n",
    "    while change:\n",
    "        change=False\n",
    "        for curNode in nodes:\n",
    "            for curNeigbour in graph.neighbors(curNode):\n",
    "                if (curNeigbour in nodes) or (curNeigbour in exclude): continue\n",
    "                nodes.append(curNeigbour)\n",
    "                change = True\n",
    "    #print [[w.word for w in x.text] for x in nodes]\n",
    "\n",
    "#     minInd = min([n.minIndex() for n in nodes])-1\n",
    "#     maxInd = max([n.maxIndex() for n in nodes])-1\n",
    "#     ret = \" \".join(graph.originalSentence.split(\" \")[minInd:maxInd+1])\n",
    "#     nodes = [n for n in minimal_spanning_tree(graph, node) if not n in exclude]\n",
    "#     ret = \" \".join(node.get_original_text() for node in sorted(nodes,key = lambda n: n.minIndex()))\n",
    "    try:\n",
    "        ret = \"\"\n",
    "        words = []\n",
    "        ids = []\n",
    "        for n in nodes:\n",
    "            words += n.surface_form\n",
    "            ids.append(n.uid)\n",
    "            # add collapsed prepositions\n",
    "            for parent_rel in n.incidents():\n",
    "                # but just nested ones\n",
    "                if parent_rel.startswith(\"prep_\") and n.incidents()[parent_rel][0] in nodes:\n",
    "                    idx = get_min_max_span(graph,n)[0] - 1\n",
    "                    w = Word(idx,parent_rel.replace(\"prep_\",\"\"))\n",
    "                    words += [w]    \n",
    "\n",
    "        words = list(set(words))\n",
    "        for w,i  in zip(words, ids):\n",
    "            print(w, i)\n",
    "        ret = \" \".join([w.word for w in strip_punctuations(sorted(words,key=lambda w:w.index))])+\" \"\n",
    "\n",
    "    except:\n",
    "        raise Exception()\n",
    "#     \n",
    "    \n",
    "    return \"schwub\"\n",
    "\n",
    "def getPropositions(self):\n",
    "    ret = []\n",
    "    for topNode in [n for n in self.nodes() if n.features.get(\"top\", False)]:\n",
    "            #print [w.word for w in topNode.text]\n",
    "#            if \"dups\" in topNode.features:\n",
    "            dups = topNode.features.get(\"dups\", [])\n",
    "            allDups = reduce(lambda x, y:list(x) + list(y), dups, [])\n",
    "\n",
    "            rest = [n for n in self.neighbors(topNode) if n not in allDups]\n",
    "            neigboursList = []\n",
    "            for combination in product(*dups):\n",
    "                curNeigbourList = []\n",
    "                ls = list(combination) + rest\n",
    "                for curNode in ls:\n",
    "                    curNeigbourList.append((self.edge_label((topNode, curNode)), curNode))\n",
    "                neigboursList.append(curNeigbourList)\n",
    "         \n",
    "            for nlist in neigboursList:\n",
    "                #print [[w.word for w in x[1].text] for x in nlist]\n",
    "                argList = []\n",
    "                all_neighbours = [n for _, n in nlist]\n",
    "                for k, curNeighbour in sorted(nlist, key=lambda k_n:get_min_max_span(self, k_n[1])[0]):\n",
    "                    curExclude = [n for n in all_neighbours if n != curNeighbour] + [topNode]\n",
    "                    if self.edge_label((topNode,curNeighbour)) != \"dep\":\n",
    "                        \n",
    "                        argList.append([k, subgraph_to_string(self, curNeighbour, exclude=curExclude), curNeighbour.uid])\n",
    "   \n",
    "                if topNode.features.get(\"Lemma\"):\n",
    "                    topNodeText = encode_german_characters(topNode.features.get(\"Lemma\"))\n",
    "                else:\n",
    "                    topNodeText = topNode.get_original_text()\n",
    "                if topNode.features.get(\"Negation\",False):\n",
    "                    topNodeText = \"nicht \" + topNodeText\n",
    "                argList = [a for a in argList if not a[1].strip() in [\"und\",\"oder\"]]\n",
    "                \n",
    "                if not len(argList) == 0:\n",
    "                    ret.append((topNode, argList))\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-53f3e158d4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "word_indices = {}\n",
    "ready = {}\n",
    "current_length = 0\n",
    "predicates = []\n",
    "\n",
    "for g, tree in gs:     \n",
    "    prop = getPropositions(g)\n",
    "    print(type(tree))\n",
    "    print(tree)\n",
    "    for arg in prop[0][1]:\n",
    "        try:\n",
    "            print(entities[arg[-1]])\n",
    "            print(arg)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "def _extract_subj_from_article_stanza(pos_tag, personal_pronouns, pronoun):\n",
    "    for sent in pos_tag.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.deprel == \"nsubj\" or word.deprel == \"root\":\n",
    "                if callable(personal_pronouns[pronoun]):\n",
    "                    _pronoun = personal_pronouns[pronoun](word.text)\n",
    "                else:\n",
    "                    _pronoun = personal_pronouns[pronoun]\n",
    "                return (_pronoun, word.lemma)\n",
    "            \n",
    "def _get_pronoun_for_den(word):\n",
    "    if word[-2:] == \"en\":\n",
    "        return \"sie\"\n",
    "    else:\n",
    "        return \"er\"\n",
    "\n",
    "def _map_article_of_subj_to_pronoun(arg):\n",
    "    personal_pronouns = {\"der\": \"er\", \"das\": \"es\", \"die\": \"sie\", \"dem\": \"er\", \"den\": _get_pronoun_for_den}\n",
    "    \n",
    "    if \"subj\" in arg:\n",
    "        words = arg[1].lower().split()\n",
    "        pronoun_place = list(word in personal_pronouns for word in words)\n",
    "        pronoun = list(compress(words, pronoun_place))\n",
    "    \n",
    "        if len(pronoun) == 1:\n",
    "            pos_tag = nlp(arg[1])\n",
    "            return _extract_subj_from_article_stanza(pos_tag, personal_pronouns, pronoun[0])\n",
    "    \n",
    "    for pronoun in personal_pronouns:\n",
    "        if pronoun in arg[1] and \"prep\" not in arg[0]:\n",
    "            pos_tag = nlp(arg[1])\n",
    "            return _extract_subj_from_article_stanza(pos_tag, personal_pronouns, pronoun)\n",
    "                        \n",
    "    return None\n",
    "\n",
    "def _get_core_from_noun_phrase(noun_phrase):\n",
    "    pos_tag = nlp(noun_phrase)\n",
    "    for sent in pos_tag.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.head == 0:\n",
    "                return {\"np\": noun_phrase, \"core\":word.lemma}\n",
    "            \n",
    "def _get_context(prop_cache, subjects, words):\n",
    "    for j in range(0, len(subjects)):\n",
    "        for k in range(len(subjects[index-j]) - 1, 0, -1):\n",
    "            if index > 0 and len(subjects[index-j]) > 0 and subjects[index-j][k][0] in words:\n",
    "                subject_index = words.index(subjects[index-j][k][0])\n",
    "                return _get_core_from_noun_phrase(subjects[index-j][k][1])[\"core\"]\n",
    "            \n",
    "def pretty_print(prop_cache):\n",
    "    for prop in prop_cache:\n",
    "        test = {}\n",
    "        for arg in prop[\"args\"]:\n",
    "            if \"context\" in arg[1]:\n",
    "                test[arg[0]] = arg[1][\"context\"]\n",
    "            else:\n",
    "                test[arg[0]] = arg[1][\"core\"]\n",
    "        prep_exists = False\n",
    "        for k in test:\n",
    "            if \"_\" in k:\n",
    "                if \"conj\" in k:\n",
    "                    pass\n",
    "                else:\n",
    "                    if \"subj\" in k:\n",
    "                        print(test[\"subj\"] + \" \" + prop[\"pred\"] + \" \" + k + \" \" + test[k])\n",
    "                prep_exists = True\n",
    "        if not prep_exists:  \n",
    "            print(test[\"subj\"] + \" \" + prop[\"pred\"] + \" \" + test[\"dobj\"])\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "subjects = []\n",
    "prop_cache = []\n",
    "index = 0\n",
    "\n",
    "for i, (g, tree) in enumerate(gs):\n",
    "    for prop in g.getPropositions('pdf'):\n",
    "        subjects.append([])\n",
    "        prop_cache.append({\"args\": prop.args, \"pred\": prop.pred})\n",
    "        \n",
    "while index < len(prop_cache):\n",
    "    for i, arg in enumerate(prop_cache[index][\"args\"]):\n",
    "        words = arg[1].lower().split()\n",
    "\n",
    "        _article_pronoun = _map_article_of_subj_to_pronoun(arg)\n",
    "        _noun_phrase_core = _get_core_from_noun_phrase(arg[1])\n",
    "        if _article_pronoun:\n",
    "            subjects[index].append(_article_pronoun)\n",
    "        if _noun_phrase_core:\n",
    "            prop_cache[index][\"args\"][i][1] = _noun_phrase_core\n",
    "        context = _get_context(prop_cache, subjects, words)\n",
    "        if context:\n",
    "            prop_cache[index][\"args\"][i][1][\"context\"] = context\n",
    "\n",
    "                    \n",
    "\n",
    "    index += 1\n",
    "pretty_print(prop_cache)\n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der regierende Bürgermeister fährt zum Rathaus.\n",
    "Im roten Rathaus geht er in den Sitzungssaal.\n",
    "Der alte Sitzungssaal liegt neben dem Büro von Michael Müller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prop in prop_cache:\n",
    "    str_build = \"\"\n",
    "    \n",
    "    for arg in prop[\"args\"]:\n",
    "        if arg[0] is not \"mod\":\n",
    "            if \"context\" in arg[1]:\n",
    "                str_build += arg[1]['context'] + \"/\"\n",
    "\n",
    "            str_build += arg[1]['core'] + \"/\"\n",
    "    print(prop[\"pred\"] + \": \" + str_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "versuchen:(subj:Die Täter , prep_in:der Nacht vom 19 )\n",
    "Juni 2014:(dobj:die Eingangstür des Bürokomplexes in der Feldtmannstraße gewaltsam zu öffnen )\n",
    "gelangt:(subj:sie , prep_in:das Objekt , mod:so )\n",
    "aufhebeln:(subj:sie , dobj:ein Fenster an der Rückseite des Hauses , mod:Als dies nicht gelang , mod:schließlich )\n",
    "und:(conj_und:Als dies nicht gelang , hebelten sie schließlich ein Fenster an der Rückseite des Hauses auf , conj_und:sie gelangten so in das Objekt )\n",
    "entwenden:(subj:sie , dobj:auch eine EC-Karte , die bereits an Freitag , 20 , prep_aus:den Büroräumen mehrerer dort ansässiger Firmen , prep_neben:Geld )\n",
    "20:(subj:auch eine EC-Karte , prep_an:Freitag , mod:bereits )\n",
    "einsetzen:(obj:Juni 2014 , prep_gegen:5.10 Uhr , prep_an:einem Geldautomaten in der Danziger Straße )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'np': 'Gestern ', 'core': 'Gestern'}\n",
      "{'np': 'Nachmittag ', 'core': 'Nachmittag'}\n",
      "{'np': 'einem schweren Verkehrsunfall in Rudow ', 'core': 'Verkehrsunfall'}\n",
      "{'np': '17:30 Uhr ', 'core': 'Uhr'}\n",
      "{'np': 'ein Opel ', 'core': 'Opel'}\n",
      "{'np': 'der Schönefelder Straße in Richtung Rudower Spinne ', 'core': 'Straße'}\n",
      "{'np': 'einen Mercedes ', 'core': 'Mercedes'}\n",
      "{'np': 'einen Mercedes ', 'core': 'Mercedes'}\n",
      "{'np': 'der Neuköllner Straße in Richtung Waltersdorfer Chaussee ', 'core': 'Straße'}\n",
      "{'np': 'unterwegs ', 'core': 'unterwegs'}\n",
      "{'np': 'Als die Fahrerin die Kreuzung Neuköllner Straße passieren wollte ', 'core': 'passieren'}\n",
      "{'np': 'den ersten Ermittlungen zufolge ', 'core': 'Ermittlung'}\n",
      "{'np': 'sie ', 'core': 'sie', 'context': 'Ermittlung'}\n",
      "{'np': 'einen Mercedes ', 'core': 'Mercedes'}\n",
      "{'np': 'einem Zusammenstoß ', 'core': 'Zusammenstoß'}\n",
      "{'np': 'Der Mercedes ', 'core': 'Mercedes'}\n",
      "{'np': 'Schleudern ', 'core': 'schleudern'}\n",
      "{'np': 'Die Feuerwehr ', 'core': 'Feuerwehr'}\n",
      "{'np': 'den 49 Jahre alten Fahrer ', 'core': 'Fahrer'}\n",
      "{'np': 'seinem Fahrzeug ', 'core': 'Fahrzeug'}\n",
      "{'np': 'bergen ', 'core': 'bergen'}\n",
      "{'np': 'Die Feuerwehr ', 'core': 'Feuerwehr'}\n",
      "{'np': 'ihn ', 'core': 'er'}\n",
      "{'np': 'stationären Behandlung ', 'core': 'Behandlung'}\n",
      "{'np': 'ein Krankenhaus ', 'core': 'Krankenhaus'}\n",
      "{'np': 'er', 'core': 'er', 'context': 'Fahrer'}\n",
      "{'np': 'Fahrzeug ', 'core': 'Fahrzeug'}\n",
      "{'np': 'Die Feuerwehr musste den 49 Jahre alten Fahrer aus seinem Fahrzeug bergen ', 'core': 'bergen'}\n",
      "{'np': 'Die Feuerwehr brachte ihn zu stationären Behandlung in ein Krankenhaus ', 'core': 'bringen'}\n",
      "{'np': 'Die 42-jährige Opel-Fahrerin ', 'core': 'Opel'}\n"
     ]
    }
   ],
   "source": [
    "for prop in prop_cache:\n",
    "    for arg in prop[\"args\"]:\n",
    "        print(arg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": \"1\",\n",
       "      \"text\": \"Die\",\n",
       "      \"lemma\": \"der\",\n",
       "      \"upos\": \"DET\",\n",
       "      \"xpos\": \"ART\",\n",
       "      \"feats\": \"Case=Nom|Definite=Def|Gender=Fem|Number=Sing|PronType=Art\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"det\",\n",
       "      \"misc\": \"start_char=0|end_char=3\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"2\",\n",
       "      \"text\": \"Feuerwehr\",\n",
       "      \"lemma\": \"Feuerwehr\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Case=Nom|Gender=Fem|Number=Sing\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"nsubj\",\n",
       "      \"misc\": \"start_char=4|end_char=13\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"3\",\n",
       "      \"text\": \"brachte\",\n",
       "      \"lemma\": \"bringen\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VVFIN\",\n",
       "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"misc\": \"start_char=14|end_char=21\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"4\",\n",
       "      \"text\": \"ihn\",\n",
       "      \"lemma\": \"er\",\n",
       "      \"upos\": \"PRON\",\n",
       "      \"xpos\": \"PPER\",\n",
       "      \"feats\": \"Case=Acc|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"obj\",\n",
       "      \"misc\": \"start_char=22|end_char=25\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"5\",\n",
       "      \"text\": \"zu\",\n",
       "      \"lemma\": \"zu\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"APPR\",\n",
       "      \"head\": 7,\n",
       "      \"deprel\": \"case\",\n",
       "      \"misc\": \"start_char=26|end_char=28\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"6\",\n",
       "      \"text\": \"stationären\",\n",
       "      \"lemma\": \"stationär\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"ADJA\",\n",
       "      \"feats\": \"Case=Dat|Gender=Fem|Number=Sing\",\n",
       "      \"head\": 7,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"misc\": \"start_char=29|end_char=40\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"7\",\n",
       "      \"text\": \"Behandlung\",\n",
       "      \"lemma\": \"Behandlung\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Case=Dat|Gender=Fem|Number=Sing\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"obl\",\n",
       "      \"misc\": \"start_char=41|end_char=51\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"8\",\n",
       "      \"text\": \"in\",\n",
       "      \"lemma\": \"in\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"APPR\",\n",
       "      \"head\": 10,\n",
       "      \"deprel\": \"case\",\n",
       "      \"misc\": \"start_char=52|end_char=54\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"9\",\n",
       "      \"text\": \"ein\",\n",
       "      \"lemma\": \"ein\",\n",
       "      \"upos\": \"DET\",\n",
       "      \"xpos\": \"ART\",\n",
       "      \"feats\": \"Case=Acc|Definite=Ind|Gender=Neut|Number=Sing|PronType=Art\",\n",
       "      \"head\": 10,\n",
       "      \"deprel\": \"det\",\n",
       "      \"misc\": \"start_char=55|end_char=58\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"10\",\n",
       "      \"text\": \"Krankenhaus\",\n",
       "      \"lemma\": \"Krankenhaus\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Case=Acc|Gender=Neut|Number=Sing\",\n",
       "      \"head\": 7,\n",
       "      \"deprel\": \"nmod\",\n",
       "      \"misc\": \"start_char=59|end_char=70\"\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('Die Feuerwehr brachte ihn zu stationären Behandlung in ein Krankenhaus ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\twas\twas\tPRO\tPIS\t_|Nom|Sg\t2\tsubj\t_\t_\n",
      "2\tgeht\tgehen\tV\tVVFIN\t3|Sg|Pres|Ind\t0\troot\t_\t_\n",
      "3\tab\tab\tPTKVZ\tPTKVZ\t_\t2\tavz\t_\t_\n",
      "4\t.\t.\t$.\t$.\t_\t0\troot\t_\t_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
