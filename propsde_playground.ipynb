{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json:   0%|          | 0.00/20.0k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 116kB [00:00, 2.69MB/s]                    \n",
      "2020-05-15 11:53:23 INFO: Downloading default packages for language: de (German)...\n",
      "2020-05-15 11:53:24 INFO: File exists: /home/chris/stanza_resources/de/default.zip.\n",
      "2020-05-15 11:53:29 INFO: Finished downloading models and saved to /home/chris/stanza_resources.\n",
      "2020-05-15 11:53:29 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | conll03 |\n",
      "=======================\n",
      "\n",
      "2020-05-15 11:53:29 INFO: Use device: gpu\n",
      "2020-05-15 11:53:29 INFO: Loading: tokenize\n",
      "2020-05-15 11:53:29 INFO: Loading: mwt\n",
      "2020-05-15 11:53:29 INFO: Loading: pos\n",
      "2020-05-15 11:53:30 INFO: Loading: lemma\n",
      "2020-05-15 11:53:30 INFO: Loading: depparse\n",
      "2020-05-15 11:53:30 INFO: Loading: ner\n",
      "2020-05-15 11:53:31 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import os, sys, codecs, time, datetime\n",
    "import fileinput\n",
    "import os.path\n",
    "import stanza\n",
    "from io import StringIO\n",
    "from subprocess import call\n",
    "\n",
    "\n",
    "from docopt import docopt\n",
    "from propsde.applications.viz_tree import DepTreeVisualizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import propsde.applications.run as run\n",
    "import sys\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str\n",
    "    \n",
    "stanza.download('de')\n",
    "nlp = stanza.Pipeline('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('polizeiberichte.pkl', 'rb') as f:\n",
    "    data = pickle.load(f, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gute Texte: 5\n",
    "Schwierige Texte: 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gestern Nachmittag kam es zu einem schweren Verkehrsunfall in Rudow. Gegen 17.30 Uhr fuhr ein „Opel“ auf der Schönefelder Straße in Richtung Rudower Spinne. Als die Fahrerin den ersten Ermittlungen zufolge die Kreuzung Neuköllner Straße passieren wollte, übersah sie offenbar einen kreuzenden „Mercedes“, der auf der Neuköllner Straße in Richtung Waltersdorfer Chaussee unterwegs war. Es kam zu einem Zusammenstoß. Der „Mercedes“ geriet ins Schleudern, kippte auf die Fahrerseite und prallte gegen einen geparkten „Seat“. Die Feuerwehr musste den 49 Jahre alten Fahrer aus seinem Fahrzeug bergen und brachte ihn zur stationären Behandlung in ein Krankenhaus. Die 42-jährige „Opel“-Fahrerin blieb unverletzt.\n"
     ]
    }
   ],
   "source": [
    "text_example = data[80][\"text1\"]\n",
    "print(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running mate-tools\n",
      "lemmatizing\n",
      "parsing + tagging\n",
      "collapsing\n",
      "java -jar ext/org.jobimtext.collapsing.jar -i tmp/parsed.conll06 -o tmp -sf -l de -r ext/resources/german_modified.txt -f c -np -nt\n",
      "UFF\n"
     ]
    }
   ],
   "source": [
    "file = codecs.open(\"examples.txt\", encoding='utf8')\n",
    "gs = run.parseSentences(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph, tree in gs:\n",
    "    original_line = str(graph).split(\"\\n\")[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['prop_of', 'Die 42-jährige Opel-Fahrerin ']]\n",
      "unverletzt\n"
     ]
    }
   ],
   "source": [
    "prop = g.getPropositions('pdf')[0]\n",
    "print(prop.args)\n",
    "print(prop.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gestern Nachmittag kam es zu einem schweren Verkehrsunfall in Rudow .\n",
      "0\t1,Gestern\tADV\t0\t0\tmod,2\n",
      "1\t2,Nachmittag\tADV\t0\t0\tmod,2\n",
      "2\t3,kam\tVVFIN\t1\t1\t\n",
      "3\t4,es\tPPER\t0\t0\tdep,2\n",
      "5\t7,schweren\tADJA\t0\t0\tmod,6\n",
      "6\t8,Verkehrsunfall\tNN\t0\t0\tprep_zu,2\n",
      "8\t10,Rudow\tNE\t0\t0\tprep_in,6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = gs[0][0]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParZU Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "def parse_dependencies_to_conll_file(text, path):\n",
    "    params = {\n",
    "        \"text\": text,\n",
    "        \"format\": \"conll\"\n",
    "    }\n",
    "    args = urllib.parse.urlencode(params).encode(\"utf-8\")\n",
    "    req = \"http://localhost:5003/parse/?\"+args.decode(\"utf-8\")\n",
    "    try:\n",
    "        f = urllib.request.urlopen(req)\n",
    "        file = open(path,'w')\n",
    "        file.write(f.read().decode('utf-8'))\n",
    "        return 0\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CorZu 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output = \"corzu_export\"\n",
    "conll = \"test.conll\"\n",
    "markables = \"markables\"\n",
    "text = open(\"examples.txt\", \"r\").read()\n",
    "\n",
    "err = parse_dependencies_to_conll_file(text, conll)\n",
    "if err == 0:\n",
    "    cmd = \"python ext/CorZu_v2.0/extract_mables_from_parzu.py \" + conll + \" > \" + markables\n",
    "    err = os.system(cmd)\n",
    "    print(err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "cmd = \"python ext/CorZu_v2.0/corzu.py \" + markables + \" \" + conll + \" \" + output    \n",
    "err = os.system(cmd)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with CorZu Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<propsde.graph_representation.newNode.Node object at 0x7ff98c6e1208>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-550-331265763903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mword_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_conll_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"-\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_original_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/uni/master/props-de/propsde/graph_representation/newNode.py\u001b[0m in \u001b[0;36mto_conll_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m                           \u001b[0;34mu\";\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_conll_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misPredicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                           \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"top\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                           \u001b[0;34mu\";\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfather\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfather\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincidents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                           ])\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/uni/master/props-de/propsde/graph_representation/graph_wrapper.py\u001b[0m in \u001b[0;36mincidents\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodesMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdigraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincidents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdigraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincidents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pygraph/classes/digraph.py\u001b[0m in \u001b[0;36mincidents\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0mdirectly\u001b[0m \u001b[0maccessible\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_incidence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: <propsde.graph_representation.newNode.Node object at 0x7ff98c6e1208>"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_np(lines, id):\n",
    "    np = \"\"\n",
    "    indices = 0\n",
    "    for line in lines:\n",
    "        line_list = re.split('\\t| +',line)\n",
    "        np += line_list[1] + \" \"\n",
    "        indices += 1\n",
    "        if line.strip().endswith(id + ')'):\n",
    "            return np, indices\n",
    "\n",
    "\n",
    "text = open(output, \"r\").read()\n",
    "lines=text.split('\\n\\n')\n",
    "\n",
    "for i in range(0, len(lines)):\n",
    "    lines[i] = lines[i].split(\"\\n\")\n",
    "    for j in range(0, len(lines[i])):\n",
    "        lines[i][j] = lines[i][j].split(\"\\t\")\n",
    "lines = {i: l for i, l in enumerate(lines) if len(l) > 0}\n",
    "\n",
    "\n",
    "for sentence_count, gt in enumerate(gs):\n",
    "    graph, tree = gt\n",
    "    word_ids = []\n",
    "    for node in graph.nodes():\n",
    "        index = int(node.to_conll_like().split(\"\\t\")[1].split(\",\")[0]) \n",
    "        if lines[sentence_count][index][-1] != \"-\" and lines[sentence_count][index][-1] != \"\":\n",
    "            print(node.get_original_text())\n",
    "            node.features[\"coreference\"] = lines[sentence_count][index][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kreuzung\n",
      "(0\n",
      "Straße\n",
      "0)\n",
      "passieren\n",
      "0)\n",
      "sie\n",
      "(0)\n",
      "kreuzenden\n",
      "(1\n",
      "Mercedes\n",
      "1)\n",
      "Mercedes\n",
      "1)\n",
      "geriet\n",
      "1)\n",
      "seinem\n",
      "(2)\n",
      "Fahrzeug\n",
      "2)\n",
      "bergen\n",
      "2)\n",
      "ihn\n",
      "(2)\n",
      "49\n",
      "(2\n",
      "haben\n",
      "(2)\n"
     ]
    }
   ],
   "source": [
    "for graph, tree in gs:\n",
    "    for node in graph:\n",
    "        if \"coreference\" in node.features:\n",
    "            print(node.text[0])\n",
    "            print(node.features[\"coreference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index = 0\n",
    "for graph, tree in gs:\n",
    "    sentences[sentence_index] = {}\n",
    "    for line in lines:\n",
    "            l = line.split('\\t')\n",
    "            line_list = re.split('\\t| +',line)\n",
    "\n",
    "            id = re.findall('\\d+',line_list[-1])\n",
    "            if id and id[0] not in entities and line.strip().endswith('(' + id[0]):\n",
    "    \n",
    "                sentences[sentence_index][l[0]] = id[0]\n",
    "                print(i, l[0], l[1])\n",
    "                entities[i] = \"(\" + id[0]\n",
    "    \n",
    "                for j in range(1, indices-2):\n",
    "         \n",
    "                    entities[i + j] = \"-\"\n",
    "                entities[i + indices] = id[0] + \")\"\n",
    "    \n",
    "            elif id and line.strip().endswith('(' + id[0] + ')'):\n",
    "                entities[i] = \"(\" + id[0] + \")\"\n",
    "    sentence_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{33: '(0', 34: '-', 37: '0)', 41: '(0)', 43: '(1', 46: '1)', 65: '(1', 67: '1)', 85: '(2', 86: '-', 87: '-', 88: '-', 89: '-', 92: '2)', 91: '(2)', 96: '(2)'}\n",
      "geriet\n"
     ]
    }
   ],
   "source": [
    "print(entities)\n",
    "print(conll_text[67])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining CorZu and PropsDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract propositions with words and their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import product\n",
    "from propsde.graph_representation.graph_utils import get_min_max_span, find_nodes, \\\n",
    "    find_edges, merge_nodes, multi_get, duplicateEdge, accessibility_wo_self, \\\n",
    "    replace_head, find_marker_idx\n",
    "from propsde.utils.utils import encode_german_characters, encode_german_chars\n",
    "from propsde.graph_representation.proposition import Proposition\n",
    "from propsde.graph_representation.word import NO_INDEX, Word, strip_punctuations\n",
    "\n",
    "def subgraph_to_string(graph,node,exclude=[]):\n",
    "    nodes = [node]\n",
    "    change = True\n",
    "    while change:\n",
    "        change=False\n",
    "        for curNode in nodes:\n",
    "            for curNeigbour in graph.neighbors(curNode):\n",
    "                if (curNeigbour in nodes) or (curNeigbour in exclude): continue\n",
    "                nodes.append(curNeigbour)\n",
    "                change = True\n",
    "    #print [[w.word for w in x.text] for x in nodes]\n",
    "\n",
    "#     minInd = min([n.minIndex() for n in nodes])-1\n",
    "#     maxInd = max([n.maxIndex() for n in nodes])-1\n",
    "#     ret = \" \".join(graph.originalSentence.split(\" \")[minInd:maxInd+1])\n",
    "#     nodes = [n for n in minimal_spanning_tree(graph, node) if not n in exclude]\n",
    "#     ret = \" \".join(node.get_original_text() for node in sorted(nodes,key = lambda n: n.minIndex()))\n",
    "    try:\n",
    "        ret = \"\"\n",
    "        words = []\n",
    "        ids = []\n",
    "        for n in nodes:\n",
    "            words += n.surface_form\n",
    "            ids.append(n.uid)\n",
    "            # add collapsed prepositions\n",
    "            for parent_rel in n.incidents():\n",
    "                # but just nested ones\n",
    "                if parent_rel.startswith(\"prep_\") and n.incidents()[parent_rel][0] in nodes:\n",
    "                    idx = get_min_max_span(graph,n)[0] - 1\n",
    "                    w = Word(idx,parent_rel.replace(\"prep_\",\"\"))\n",
    "                    words += [w]    \n",
    "\n",
    "        words = list(set(words))\n",
    "        for w,i  in zip(words, ids):\n",
    "            print(w, i)\n",
    "        ret = \" \".join([w.word for w in strip_punctuations(sorted(words,key=lambda w:w.index))])+\" \"\n",
    "\n",
    "    except:\n",
    "        raise Exception()\n",
    "#     \n",
    "    \n",
    "    return \"schwub\"\n",
    "\n",
    "def getPropositions(self):\n",
    "    ret = []\n",
    "    for topNode in [n for n in self.nodes() if n.features.get(\"top\", False)]:\n",
    "            #print [w.word for w in topNode.text]\n",
    "#            if \"dups\" in topNode.features:\n",
    "            dups = topNode.features.get(\"dups\", [])\n",
    "            allDups = reduce(lambda x, y:list(x) + list(y), dups, [])\n",
    "\n",
    "            rest = [n for n in self.neighbors(topNode) if n not in allDups]\n",
    "            neigboursList = []\n",
    "            for combination in product(*dups):\n",
    "                curNeigbourList = []\n",
    "                ls = list(combination) + rest\n",
    "                for curNode in ls:\n",
    "                    curNeigbourList.append((self.edge_label((topNode, curNode)), curNode))\n",
    "                neigboursList.append(curNeigbourList)\n",
    "         \n",
    "            for nlist in neigboursList:\n",
    "                #print [[w.word for w in x[1].text] for x in nlist]\n",
    "                argList = []\n",
    "                all_neighbours = [n for _, n in nlist]\n",
    "                for k, curNeighbour in sorted(nlist, key=lambda k_n:get_min_max_span(self, k_n[1])[0]):\n",
    "                    curExclude = [n for n in all_neighbours if n != curNeighbour] + [topNode]\n",
    "                    if self.edge_label((topNode,curNeighbour)) != \"dep\":\n",
    "                        \n",
    "                        argList.append([k, subgraph_to_string(self, curNeighbour, exclude=curExclude), curNeighbour.uid])\n",
    "   \n",
    "                if topNode.features.get(\"Lemma\"):\n",
    "                    topNodeText = encode_german_characters(topNode.features.get(\"Lemma\"))\n",
    "                else:\n",
    "                    topNodeText = topNode.get_original_text()\n",
    "                if topNode.features.get(\"Negation\",False):\n",
    "                    topNodeText = \"nicht \" + topNodeText\n",
    "                argList = [a for a in argList if not a[1].strip() in [\"und\",\"oder\"]]\n",
    "                \n",
    "                if not len(argList) == 0:\n",
    "                    ret.append((topNode, argList))\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gestern 669\n",
      "Nachmittag 670\n",
      "Verkehrsunfall 675\n",
      "in 677\n",
      "einem 674\n",
      "<class 'str'>\n",
      "1\tGestern\t_\tADV\tADV\t_\t3\tMO\t_\t_\n",
      "2\tNachmittag\t_\tADV\tADV\t_\t3\tMO\t_\t_\n",
      "3\tkam\t_\tVVFIN\tVVFIN\t_\t0\t--\t_\t_\n",
      "4\tes\t_\tPPER\tPPER\t_\t3\tEP\t_\t_\n",
      "6\teinem\t_\tART\tART\t_\t8\tNK\t_\t_\n",
      "7\tschweren\t_\tADJA\tADJA\t_\t8\tNK\t_\t_\n",
      "8\tVerkehrsunfall\t_\tNN\tNN\t_\t3\tMO_zu\t_\t_\n",
      "9\tin\t_\tAPPR\tAPPR\t_\t8\tMNR\t_\t_\n",
      "10\tRudow\t_\tNE\tNE\t_\t9\tNK\t_\t_\n",
      "11\t.\t_\t$.\t$.\t_\t10\t--\t_\t_\n",
      "\n",
      "17.30 773\n",
      "ein 684\n",
      "Spinne 687\n",
      "der 689\n",
      "in 686\n",
      ". 774\n",
      "<class 'str'>\n",
      "2\t17.30\t_\tCARD\tCARD\t_\t3\tNK\t_\t_\n",
      "3\tUhr\t_\tNN\tNN\t_\t4\tMO_gegen\t_\t_\n",
      "4\tfuhr\t_\tVVFIN\tVVFIN\t_\t0\t--\t_\t_\n",
      "5\tein\t_\tART\tART\t_\t6\tNK\t_\t_\n",
      "6\tOpel\t_\tNN\tNN\t_\t4\tSB\t_\t_\n",
      "8\tder\t_\tART\tART\t_\t10\tNK\t_\t_\n",
      "9\tSchönefelder\t_\tADJA\tADJA\t_\t10\tNK\t_\t_\n",
      "10\tStraße\t_\tNN\tNN\t_\t4\tMO_auf\t_\t_\n",
      "11\tin\t_\tAPPR\tAPPR\t_\t10\tMNR\t_\t_\n",
      "12\tRichtung\t_\tNN\tNN\t_\t11\tNK\t_\t_\n",
      "13\tRudower\t_\tNE\tNE\t_\t14\tPNC\t_\t_\n",
      "14\tSpinne\t_\tNE\tNE\t_\t12\tNK\t_\t_\n",
      "15\t.\t_\t$.\t$.\t_\t14\t--\t_\t_\n",
      "\n",
      ", 713\n",
      ", 713\n",
      "der 718\n",
      "Waltersdorfer 720\n",
      "in 717\n",
      "Straße 775\n",
      "unterwegs 723\n",
      "Kreuzung 705\n",
      ", 704\n",
      "die 694\n",
      "die 696\n",
      "passieren 702\n",
      "Straße 703\n",
      "den 700\n",
      "zufolge 699\n",
      "Ermittlungen 698\n",
      "sie 709\n",
      ", 713\n",
      "<class 'str'>\n",
      "1\tAls\t_\tKOUS\tKOUS\t_\t13\tCP\t_\t_\n",
      "2\tdie\t_\tART\tART\t_\t3\tNK\t_\t_\n",
      "3\tFahrerin\t_\tNN\tNN\t_\t13\tSB\t_\t_\n",
      "4\tden\t_\tART\tART\t_\t6\tNK\t_\t_\n",
      "5\tersten\t_\tADJA\tADJA\t_\t6\tNK\t_\t_\n",
      "6\tErmittlungen\t_\tNN\tNN\t_\t7\tNK\t_\t_\n",
      "7\tzufolge\t_\tAPPO\tAPPO\t_\t15\tMO\t_\t_\n",
      "8\tdie\t_\tART\tART\t_\t9\tNK\t_\t_\n",
      "9\tKreuzung\t_\tNN\tNN\t_\t13\tSB\t_\t_\n",
      "10\tNeuköllner\t_\tADJA\tADJA\t_\t11\tNK\t_\t_\n",
      "11\tStraße\t_\tNN\tNN\t_\t12\tOA\t_\t_\n",
      "12\tpassieren\t_\tVVINF\tVVINF\t_\t13\tOC\t_\t_\n",
      "13\twollte\t_\tVMFIN\tVMFIN\t_\t15\tMO\t_\t_\n",
      "14\t,\t_\t$,\t$,\t_\t13\t--\t_\t_\n",
      "15\tübersah\t_\tVVFIN\tVVFIN\t_\t0\t--\t_\t_\n",
      "16\tsie\t_\tPPER\tPPER\t_\t15\tSB\t_\t_\n",
      "17\toffenbar\t_\tADJD\tADJD\t_\t15\tMO\t_\t_\n",
      "18\teinen\t_\tART\tART\t_\t20\tNK\t_\t_\n",
      "19\tkreuzenden\t_\tADJA\tADJA\t_\t20\tNK\t_\t_\n",
      "20\tMercedes\t_\tNE\tNE\t_\t15\tOA\t_\t_\n",
      "21\t,\t_\t$,\t$,\t_\t20\t--\t_\t_\n",
      "22\tder\t_\tPRELS\tPRELS\t_\t32\tSB\t_\t_\n",
      "24\tder\t_\tART\tART\t_\t26\tNK\t_\t_\n",
      "25\tNeuköllner\t_\tADJA\tADJA\t_\t26\tNK\t_\t_\n",
      "26\tStraße\t_\tNN\tNN\t_\t32\tMO_auf\t_\t_\n",
      "27\tin\t_\tAPPR\tAPPR\t_\t26\tMNR\t_\t_\n",
      "28\tRichtung\t_\tNN\tNN\t_\t27\tNK\t_\t_\n",
      "29\tWaltersdorfer\t_\tNE\tNE\t_\t30\tPNC\t_\t_\n",
      "30\tChaussee\t_\tNE\tNE\t_\t28\tNK\t_\t_\n",
      "31\tunterwegs\t_\tADV\tADV\t_\t32\tMO\t_\t_\n",
      "32\twar\t_\tVAFIN\tVAFIN\t_\t20\tRC\t_\t_\n",
      "33\t.\t_\t$.\t$.\t_\t32\t--\t_\t_\n",
      "\n",
      "Zusammenstoß 730\n",
      "<class 'str'>\n",
      "1\tEs\t_\tPPER\tPPER\t_\t2\tEP\t_\t_\n",
      "2\tkam\t_\tVVFIN\tVVFIN\t_\t0\t--\t_\t_\n",
      "4\teinem\t_\tART\tART\t_\t5\tNK\t_\t_\n",
      "5\tZusammenstoß\t_\tNN\tNN\t_\t2\tMO_zu\t_\t_\n",
      "6\t.\t_\t$.\t$.\t_\t5\t--\t_\t_\n",
      "\n",
      "Der 734\n",
      "Schleudern 736\n",
      "<class 'str'>\n",
      "1\tDer\t_\tART\tART\t_\t2\tNK\t_\t_\n",
      "2\tMercedes\t_\tNE\tNE\t_\t3\tSB\t_\t_\n",
      "3\tgeriet\t_\tVVFIN\tVVFIN\t_\t0\t--\t_\t_\n",
      "5\tSchleudern\t_\tNN\tNN\t_\t3\tMO_in\t_\t_\n",
      "6\t,\t_\t$,\t$,\t_\t5\t--\t_\t_\n",
      "7\tkippte\t_\tVVFIN\tVVFIN\t_\t3\tCJ\t_\t_\n",
      "9\tdie\t_\tART\tART\t_\t10\tNK\t_\t_\n",
      "10\tFahrerseite\t_\tNN\tNN\t_\t7\tMO_auf\t_\t_\n",
      "12\tprallte\t_\tVVFIN\tVVFIN\t_\t7\tCJ_und\t_\t_\n",
      "14\teinen\t_\tART\tART\t_\t16\tNK\t_\t_\n",
      "15\tgeparkten\t_\tADJA\tADJA\t_\t16\tNK\t_\t_\n",
      "16\tSeat\t_\tNN\tNN\t_\t12\tMO_gegen\t_\t_\n",
      "17\t.\t_\t$.\t$.\t_\t16\t--\t_\t_\n",
      "\n",
      "Feuerwehr 748\n",
      "Jahre 754\n",
      "den 779\n",
      "seinem 756\n",
      "Fahrzeug 755\n",
      "bergen 757\n",
      "Feuerwehr 748\n",
      "ihn 759\n",
      "stationären 761\n",
      "Behandlung 760\n",
      "ein 763\n",
      "seinem 755\n",
      "Fahrzeug 756\n",
      "Jahre 749\n",
      "bergen 756\n",
      "musste 748\n",
      "den 754\n",
      "aus 757\n",
      "Die 755\n",
      "Fahrer 779\n",
      "Behandlung 758\n",
      "in 761\n",
      "Die 763\n",
      ". 748\n",
      "Feuerwehr 759\n",
      "ein 760\n",
      "<class 'str'>\n",
      "1\tDie\t_\tART\tART\t_\t2\tNK\t_\t_\n",
      "2\tFeuerwehr\t_\tNN\tNN\t_\t3\tSB\t_\t_\n",
      "3\tmusste\t_\tVVFIN\tVVFIN\t_\t0\t--\t_\t_\n",
      "4\tden\t_\tART\tART\t_\t8\tNK\t_\t_\n",
      "5\t49\t_\tCARD\tCARD\t_\t6\tNK\t_\t_\n",
      "6\tJahre\t_\tNN\tNN\t_\t7\tAMS\t_\t_\n",
      "7\talten\t_\tADJA\tADJA\t_\t8\tNK\t_\t_\n",
      "8\tFahrer\t_\tNN\tNN\t_\t3\tOA\t_\t_\n",
      "10\tseinem\t_\tPPOSAT\tPPOSAT\t_\t11\tNK\t_\t_\n",
      "11\tFahrzeug\t_\tNN\tNN\t_\t3\tMO_aus\t_\t_\n",
      "12\tbergen\t_\tADV\tADV\t_\t3\tMO\t_\t_\n",
      "14\tbrachte\t_\tVVFIN\tVVFIN\t_\t3\tCJ_und\t_\t_\n",
      "15\tihn\t_\tPPER\tPPER\t_\t14\tOA\t_\t_\n",
      "17\tstationären\t_\tADJA\tADJA\t_\t18\tNK\t_\t_\n",
      "18\tBehandlung\t_\tNN\tNN\t_\t14\tMO_zu\t_\t_\n",
      "20\tein\t_\tART\tART\t_\t21\tNK\t_\t_\n",
      "21\tKrankenhaus\t_\tNN\tNN\t_\t14\tMO_in\t_\t_\n",
      "22\t.\t_\t$.\t$.\t_\t21\t--\t_\t_\n",
      "\n",
      "42-jährige 768\n",
      "Opel-Fahrerin 767\n",
      "<class 'str'>\n",
      "1\tDie\t_\tART\tART\t_\t3\tNK\t_\t_\n",
      "2\t42-jährige\t_\tADJA\tADJA\t_\t3\tNK\t_\t_\n",
      "3\tOpel-Fahrerin\t_\tNN\tNN\t_\t4\tSB\t_\t_\n",
      "4\tblieb\t_\tVVFIN\tVVFIN\t_\t0\t--\t_\t_\n",
      "5\tunverletzt\t_\tADJD\tADJD\t_\t4\tPD\t_\t_\n",
      "6\t.\t_\t$.\t$.\t_\t5\t--\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_indices = {}\n",
    "ready = {}\n",
    "current_length = 0\n",
    "predicates = []\n",
    "\n",
    "for g, tree in gs:     \n",
    "    prop = getPropositions(g)\n",
    "    print(type(tree))\n",
    "    print(tree)\n",
    "    for arg in prop[0][1]:\n",
    "        try:\n",
    "            print(entities[arg[-1]])\n",
    "            print(arg)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "def _extract_subj_from_article_stanza(pos_tag, personal_pronouns, pronoun):\n",
    "    for sent in pos_tag.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.deprel == \"nsubj\" or word.deprel == \"root\":\n",
    "                if callable(personal_pronouns[pronoun]):\n",
    "                    _pronoun = personal_pronouns[pronoun](word.text)\n",
    "                else:\n",
    "                    _pronoun = personal_pronouns[pronoun]\n",
    "                return (_pronoun, word.lemma)\n",
    "            \n",
    "def _get_pronoun_for_den(word):\n",
    "    if word[-2:] == \"en\":\n",
    "        return \"sie\"\n",
    "    else:\n",
    "        return \"er\"\n",
    "\n",
    "def _map_article_of_subj_to_pronoun(arg):\n",
    "    personal_pronouns = {\"der\": \"er\", \"das\": \"es\", \"die\": \"sie\", \"dem\": \"er\", \"den\": _get_pronoun_for_den}\n",
    "    \n",
    "    if \"subj\" in arg:\n",
    "        words = arg[1].lower().split()\n",
    "        pronoun_place = list(word in personal_pronouns for word in words)\n",
    "        pronoun = list(compress(words, pronoun_place))\n",
    "    \n",
    "        if len(pronoun) == 1:\n",
    "            pos_tag = nlp(arg[1])\n",
    "            return _extract_subj_from_article_stanza(pos_tag, personal_pronouns, pronoun[0])\n",
    "    \n",
    "    for pronoun in personal_pronouns:\n",
    "        if pronoun in arg[1] and \"prep\" not in arg[0]:\n",
    "            pos_tag = nlp(arg[1])\n",
    "            return _extract_subj_from_article_stanza(pos_tag, personal_pronouns, pronoun)\n",
    "                        \n",
    "    return None\n",
    "\n",
    "def _get_core_from_noun_phrase(noun_phrase):\n",
    "    pos_tag = nlp(noun_phrase)\n",
    "    for sent in pos_tag.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.head == 0:\n",
    "                return {\"np\": noun_phrase, \"core\":word.lemma}\n",
    "            \n",
    "def _get_context(prop_cache, subjects, words):\n",
    "    for j in range(0, len(subjects)):\n",
    "        for k in range(len(subjects[index-j]) - 1, 0, -1):\n",
    "            if index > 0 and len(subjects[index-j]) > 0 and subjects[index-j][k][0] in words:\n",
    "                subject_index = words.index(subjects[index-j][k][0])\n",
    "                return _get_core_from_noun_phrase(subjects[index-j][k][1])[\"core\"]\n",
    "            \n",
    "def pretty_print(prop_cache):\n",
    "    for prop in prop_cache:\n",
    "        test = {}\n",
    "        for arg in prop[\"args\"]:\n",
    "            if \"context\" in arg[1]:\n",
    "                test[arg[0]] = arg[1][\"context\"]\n",
    "            else:\n",
    "                test[arg[0]] = arg[1][\"core\"]\n",
    "        prep_exists = False\n",
    "        for k in test:\n",
    "            if \"_\" in k:\n",
    "                if \"conj\" in k:\n",
    "                    pass\n",
    "                else:\n",
    "                    if \"subj\" in k:\n",
    "                        print(test[\"subj\"] + \" \" + prop[\"pred\"] + \" \" + k + \" \" + test[k])\n",
    "                prep_exists = True\n",
    "        if not prep_exists:  \n",
    "            print(test[\"subj\"] + \" \" + prop[\"pred\"] + \" \" + test[\"dobj\"])\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ermittlung offenbar übersehen Mercedes\n",
      "Fahrer haben Fahrzeug\n",
      "[[], [], [], [], [('sie', 'Fahrerin'), ('sie', 'Ermittlung')], [], [('er', 'Mercedes')], [('sie', 'Feuerwehr'), ('er', 'Fahrer')], [('sie', 'Feuerwehr')], [], [('er', 'Feuerwehr')], []]\n"
     ]
    }
   ],
   "source": [
    "subjects = []\n",
    "prop_cache = []\n",
    "index = 0\n",
    "\n",
    "for i, (g, tree) in enumerate(gs):\n",
    "    for prop in g.getPropositions('pdf'):\n",
    "        subjects.append([])\n",
    "        prop_cache.append({\"args\": prop.args, \"pred\": prop.pred})\n",
    "        \n",
    "while index < len(prop_cache):\n",
    "    for i, arg in enumerate(prop_cache[index][\"args\"]):\n",
    "        words = arg[1].lower().split()\n",
    "\n",
    "        _article_pronoun = _map_article_of_subj_to_pronoun(arg)\n",
    "        _noun_phrase_core = _get_core_from_noun_phrase(arg[1])\n",
    "        if _article_pronoun:\n",
    "            subjects[index].append(_article_pronoun)\n",
    "        if _noun_phrase_core:\n",
    "            prop_cache[index][\"args\"][i][1] = _noun_phrase_core\n",
    "        context = _get_context(prop_cache, subjects, words)\n",
    "        if context:\n",
    "            prop_cache[index][\"args\"][i][1][\"context\"] = context\n",
    "\n",
    "                    \n",
    "\n",
    "    index += 1\n",
    "pretty_print(prop_cache)\n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der regierende Bürgermeister fährt zum Rathaus.\n",
    "Im roten Rathaus geht er in den Sitzungssaal.\n",
    "Der alte Sitzungssaal liegt neben dem Büro von Michael Müller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kommen: Verkehrsunfall/\n",
      "fahren: Uhr/Opel/Straße/\n",
      "kreuzenden: Mercedes/\n",
      "sein: Mercedes/Straße/\n",
      "offenbar übersehen: Ermittlung/sie/Mercedes/\n",
      "kommen: Zusammenstoß/\n",
      "geraten: Mercedes/schleudern/\n",
      "mussen: Feuerwehr/Fahrer/Fahrzeug/\n",
      "bringen: Feuerwehr/er/Behandlung/Krankenhaus/\n",
      "haben: Fahrer/er/Fahrzeug/\n",
      "und: bergen/bringen/\n",
      "unverletzt: Opel/\n"
     ]
    }
   ],
   "source": [
    "for prop in prop_cache:\n",
    "    str_build = \"\"\n",
    "    \n",
    "    for arg in prop[\"args\"]:\n",
    "        if arg[0] is not \"mod\":\n",
    "            if \"context\" in arg[1]:\n",
    "                str_build += arg[1]['context'] + \"/\"\n",
    "\n",
    "            str_build += arg[1]['core'] + \"/\"\n",
    "    print(prop[\"pred\"] + \": \" + str_build)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "versuchen:(subj:Die Täter , prep_in:der Nacht vom 19 )\n",
    "Juni 2014:(dobj:die Eingangstür des Bürokomplexes in der Feldtmannstraße gewaltsam zu öffnen )\n",
    "gelangt:(subj:sie , prep_in:das Objekt , mod:so )\n",
    "aufhebeln:(subj:sie , dobj:ein Fenster an der Rückseite des Hauses , mod:Als dies nicht gelang , mod:schließlich )\n",
    "und:(conj_und:Als dies nicht gelang , hebelten sie schließlich ein Fenster an der Rückseite des Hauses auf , conj_und:sie gelangten so in das Objekt )\n",
    "entwenden:(subj:sie , dobj:auch eine EC-Karte , die bereits an Freitag , 20 , prep_aus:den Büroräumen mehrerer dort ansässiger Firmen , prep_neben:Geld )\n",
    "20:(subj:auch eine EC-Karte , prep_an:Freitag , mod:bereits )\n",
    "einsetzen:(obj:Juni 2014 , prep_gegen:5.10 Uhr , prep_an:einem Geldautomaten in der Danziger Straße )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'np': 'Gestern ', 'core': 'Gestern'}\n",
      "{'np': 'Nachmittag ', 'core': 'Nachmittag'}\n",
      "{'np': 'einem schweren Verkehrsunfall in Rudow ', 'core': 'Verkehrsunfall'}\n",
      "{'np': '17:30 Uhr ', 'core': 'Uhr'}\n",
      "{'np': 'ein Opel ', 'core': 'Opel'}\n",
      "{'np': 'der Schönefelder Straße in Richtung Rudower Spinne ', 'core': 'Straße'}\n",
      "{'np': 'einen Mercedes ', 'core': 'Mercedes'}\n",
      "{'np': 'einen Mercedes ', 'core': 'Mercedes'}\n",
      "{'np': 'der Neuköllner Straße in Richtung Waltersdorfer Chaussee ', 'core': 'Straße'}\n",
      "{'np': 'unterwegs ', 'core': 'unterwegs'}\n",
      "{'np': 'Als die Fahrerin die Kreuzung Neuköllner Straße passieren wollte ', 'core': 'passieren'}\n",
      "{'np': 'den ersten Ermittlungen zufolge ', 'core': 'Ermittlung'}\n",
      "{'np': 'sie ', 'core': 'sie', 'context': 'Ermittlung'}\n",
      "{'np': 'einen Mercedes ', 'core': 'Mercedes'}\n",
      "{'np': 'einem Zusammenstoß ', 'core': 'Zusammenstoß'}\n",
      "{'np': 'Der Mercedes ', 'core': 'Mercedes'}\n",
      "{'np': 'Schleudern ', 'core': 'schleudern'}\n",
      "{'np': 'Die Feuerwehr ', 'core': 'Feuerwehr'}\n",
      "{'np': 'den 49 Jahre alten Fahrer ', 'core': 'Fahrer'}\n",
      "{'np': 'seinem Fahrzeug ', 'core': 'Fahrzeug'}\n",
      "{'np': 'bergen ', 'core': 'bergen'}\n",
      "{'np': 'Die Feuerwehr ', 'core': 'Feuerwehr'}\n",
      "{'np': 'ihn ', 'core': 'er'}\n",
      "{'np': 'stationären Behandlung ', 'core': 'Behandlung'}\n",
      "{'np': 'ein Krankenhaus ', 'core': 'Krankenhaus'}\n",
      "{'np': 'er', 'core': 'er', 'context': 'Fahrer'}\n",
      "{'np': 'Fahrzeug ', 'core': 'Fahrzeug'}\n",
      "{'np': 'Die Feuerwehr musste den 49 Jahre alten Fahrer aus seinem Fahrzeug bergen ', 'core': 'bergen'}\n",
      "{'np': 'Die Feuerwehr brachte ihn zu stationären Behandlung in ein Krankenhaus ', 'core': 'bringen'}\n",
      "{'np': 'Die 42-jährige Opel-Fahrerin ', 'core': 'Opel'}\n"
     ]
    }
   ],
   "source": [
    "for prop in prop_cache:\n",
    "    for arg in prop[\"args\"]:\n",
    "        print(arg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": \"1\",\n",
       "      \"text\": \"Die\",\n",
       "      \"lemma\": \"der\",\n",
       "      \"upos\": \"DET\",\n",
       "      \"xpos\": \"ART\",\n",
       "      \"feats\": \"Case=Nom|Definite=Def|Gender=Fem|Number=Sing|PronType=Art\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"det\",\n",
       "      \"misc\": \"start_char=0|end_char=3\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"2\",\n",
       "      \"text\": \"Feuerwehr\",\n",
       "      \"lemma\": \"Feuerwehr\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Case=Nom|Gender=Fem|Number=Sing\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"nsubj\",\n",
       "      \"misc\": \"start_char=4|end_char=13\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"3\",\n",
       "      \"text\": \"brachte\",\n",
       "      \"lemma\": \"bringen\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VVFIN\",\n",
       "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"misc\": \"start_char=14|end_char=21\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"4\",\n",
       "      \"text\": \"ihn\",\n",
       "      \"lemma\": \"er\",\n",
       "      \"upos\": \"PRON\",\n",
       "      \"xpos\": \"PPER\",\n",
       "      \"feats\": \"Case=Acc|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"obj\",\n",
       "      \"misc\": \"start_char=22|end_char=25\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"5\",\n",
       "      \"text\": \"zu\",\n",
       "      \"lemma\": \"zu\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"APPR\",\n",
       "      \"head\": 7,\n",
       "      \"deprel\": \"case\",\n",
       "      \"misc\": \"start_char=26|end_char=28\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"6\",\n",
       "      \"text\": \"stationären\",\n",
       "      \"lemma\": \"stationär\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"ADJA\",\n",
       "      \"feats\": \"Case=Dat|Gender=Fem|Number=Sing\",\n",
       "      \"head\": 7,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"misc\": \"start_char=29|end_char=40\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"7\",\n",
       "      \"text\": \"Behandlung\",\n",
       "      \"lemma\": \"Behandlung\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Case=Dat|Gender=Fem|Number=Sing\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"obl\",\n",
       "      \"misc\": \"start_char=41|end_char=51\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"8\",\n",
       "      \"text\": \"in\",\n",
       "      \"lemma\": \"in\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"APPR\",\n",
       "      \"head\": 10,\n",
       "      \"deprel\": \"case\",\n",
       "      \"misc\": \"start_char=52|end_char=54\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"9\",\n",
       "      \"text\": \"ein\",\n",
       "      \"lemma\": \"ein\",\n",
       "      \"upos\": \"DET\",\n",
       "      \"xpos\": \"ART\",\n",
       "      \"feats\": \"Case=Acc|Definite=Ind|Gender=Neut|Number=Sing|PronType=Art\",\n",
       "      \"head\": 10,\n",
       "      \"deprel\": \"det\",\n",
       "      \"misc\": \"start_char=55|end_char=58\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": \"10\",\n",
       "      \"text\": \"Krankenhaus\",\n",
       "      \"lemma\": \"Krankenhaus\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Case=Acc|Gender=Neut|Number=Sing\",\n",
       "      \"head\": 7,\n",
       "      \"deprel\": \"nmod\",\n",
       "      \"misc\": \"start_char=59|end_char=70\"\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('Die Feuerwehr brachte ihn zu stationären Behandlung in ein Krankenhaus ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\twas\twas\tPRO\tPIS\t_|Nom|Sg\t2\tsubj\t_\t_\n",
      "2\tgeht\tgehen\tV\tVVFIN\t3|Sg|Pres|Ind\t0\troot\t_\t_\n",
      "3\tab\tab\tPTKVZ\tPTKVZ\t_\t2\tavz\t_\t_\n",
      "4\t.\t.\t$.\t$.\t_\t0\troot\t_\t_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
